# Tight Loop Workflow - Status Dashboard

**Purpose:** Monitor and track tight loop iterations in real-time

---

## Session Dashboard Template

### Current Session Status

```text
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    TIGHT LOOP SESSION STATUS                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š SESSION SUMMARY
â”œâ”€ Session ID: [TIMESTAMP]
â”œâ”€ Start Time: [HH:MM:SS]
â”œâ”€ Current Iteration: N
â”œâ”€ Status: [ACTIVE | PAUSED | COMPLETE]
â””â”€ Total Duration: [MM:SS]

ğŸ¯ ITERATION PROGRESS

Iteration 1 (COMPLETE)
â”œâ”€ Plan: âœ“ 3 subtasks created
â”œâ”€ Execute: âœ“ 3/3 subtasks done
â”œâ”€ Review: âœ“ 2 improvements found
â””â”€ Discovered: "Rate limiting", "Error logging"

Iteration 2 (IN PROGRESS)
â”œâ”€ Plan: â—‹ Analyzing task...
â”œâ”€ Execute: [NOT STARTED]
â”œâ”€ Review: [NOT STARTED]
â””â”€ Discovered: [PENDING]

Iteration 3 (QUEUED)
â”œâ”€ Plan: [PENDING]
â”œâ”€ Execute: [PENDING]
â”œâ”€ Review: [PENDING]
â””â”€ Discovered: [TBD]

ğŸ“ˆ AGGREGATED METRICS
â”œâ”€ Total Tasks Started: 7
â”œâ”€ Tasks Completed: 6
â”œâ”€ Tasks Failed: 0
â”œâ”€ Tasks In Progress: 1
â”œâ”€ Tasks Pending: 2
â”œâ”€ Improvements Discovered: 4
â”œâ”€ Duplicates Prevented: 2
â””â”€ Total Time Spent: [MM:SS]

â±ï¸ PHASE TIMING
â”œâ”€ Plan Phase (Avg): [SS]
â”œâ”€ Execute Phase (Avg): [SS]
â”œâ”€ Review Phase (Avg): [SS]
â””â”€ Loop Cycle (Avg): [SS]

ğŸ”„ CURRENT PHASE
â”œâ”€ Active: Smart Plan
â”œâ”€ Step: 3 of 9 (Analyze Vagueness)
â”œâ”€ Time in Phase: [SS]
â”œâ”€ Next Decision: Ready to execute? [AWAITING USER]
â””â”€ Estimated Time: [SS remaining]

ğŸ“‹ USER INTERACTION LOG
â”œâ”€ [HH:MM:SS] Clicked: ğŸ¯ Plan Phase
â”œâ”€ [HH:MM:SS] Confirmed: Vagueness score = 0.6
â”œâ”€ [HH:MM:SS] Answered: Clarifying question about scope
â”œâ”€ [HH:MM:SS] Confirmed: Ready to execute? [YES]
â”œâ”€ [HH:MM:SS] Confirmed: OAuth setup done? [YES]
â”œâ”€ [HH:MM:SS] Confirmed: JWT management done? [YES]
â”œâ”€ [HH:MM:SS] Confirmed: Ready for review? [YES]
â”œâ”€ [HH:MM:SS] Confirmed: Add improvements? [YES]
â”œâ”€ [HH:MM:SS] Confirmed: Continue loop? [YES]
â””â”€ [HH:MM:SS] WAITING FOR: Plan phase confirmation...

ğŸ¯ NEXT ACTIONS
â”œâ”€ Immediate: User must confirm plan ready for execution
â”œâ”€ Decision Point: "Ready to execute? [YES/NO]"
â”œâ”€ If YES: Auto-handoff to Smart Execute
â”œâ”€ If NO: Return to planning (refine subtasks)
â””â”€ Estimated wait: < 1 minute
```

---

## Iteration Timeline

### Session Overview

```text
Session Start
     â”‚
     â”œâ”€ 00:00-00:30 ITERATION 1 (Plan Phase)
     â”‚  â”œâ”€ Load context: 2s
     â”‚  â”œâ”€ Call getNextTask(): 1s
     â”‚  â”œâ”€ Analyze vagueness: 5s
     â”‚  â”œâ”€ Ask clarifications: 12s (user interaction)
     â”‚  â”œâ”€ Create subtasks: 5s
     â”‚  â””â”€ Confirm plan: 5s (user: YES)
     â”‚
     â”œâ”€ 00:30-01:15 ITERATION 1 (Execute Phase)
     â”‚  â”œâ”€ Load context: 2s
     â”‚  â”œâ”€ Get subtasks: 1s
     â”‚  â”œâ”€ Execute Task 1: 15s
     â”‚  â”œâ”€ Confirm Task 1: 3s (user: YES)
     â”‚  â”œâ”€ Execute Task 2: 12s
     â”‚  â”œâ”€ Confirm Task 2: 3s (user: YES)
     â”‚  â”œâ”€ Execute Task 3: 12s
     â”‚  â”œâ”€ Confirm Task 3: 3s (user: YES)
     â”‚  â””â”€ Confirm review ready: 7s (user: YES)
     â”‚
     â”œâ”€ 01:15-02:00 ITERATION 1 (Review Phase)
     â”‚  â”œâ”€ Load context: 2s
     â”‚  â”œâ”€ List tasks: 2s
     â”‚  â”œâ”€ Analyze patterns: 15s
     â”‚  â”œâ”€ Root-cause analysis: 12s
     â”‚  â”œâ”€ Discover improvements: 8s
     â”‚  â”œâ”€ Check duplicates: 3s
     â”‚  â””â”€ Confirm add tasks: 3s (user: YES)
     â”‚
     â”œâ”€ 02:00-02:15 ITERATION 1 (Loop Decision)
     â”‚  â””â”€ Continue? (user: YES)
     â”‚
     â”œâ”€ 02:15-02:35 ITERATION 2 (Plan Phase)
     â”‚  â”œâ”€ Load context: 2s
     â”‚  â”œâ”€ Call getNextTask(): 1s (finds different task)
     â”‚  â”œâ”€ Analyze: 8s
     â”‚  â”œâ”€ Create subtasks: 4s
     â”‚  â””â”€ Confirm: 6s (user: YES)
     â”‚
     â”œâ”€ 02:35-03:30 ITERATION 2 (Execute Phase)
     â”‚  â””â”€ [Similar pattern]
     â”‚
     â”œâ”€ 03:30-04:20 ITERATION 2 (Review Phase)
     â”‚  â””â”€ [Similar pattern]
     â”‚
     â”œâ”€ 04:20-04:25 ITERATION 2 (Loop Decision)
     â”‚  â””â”€ Continue? (user: NO â† Loop Breaks)
     â”‚
     â””â”€ 04:25-04:30 SESSION SUMMARY
        â”œâ”€ Total time: 4:30
        â”œâ”€ Iterations: 2
        â”œâ”€ Tasks completed: 6
        â””â”€ Improvements: 4
```

---

## Metrics & Analytics

### Performance Metrics

```text
TIMING ANALYSIS

Average Phase Duration:
â”œâ”€ Plan Phase: 30s (range: 20-45s)
â”œâ”€ Execute Phase: 45s (range: 30-75s)
â”œâ”€ Review Phase: 45s (range: 35-60s)
â””â”€ Loop Cycle: 120s (2 minutes per iteration)

Bottleneck Analysis:
â”œâ”€ User wait times: 18% of session
â”œâ”€ System processing: 82% of session
â”œâ”€ Slowest component: Execute (waiting for per-task confirmation)
â”œâ”€ Fastest component: Plan (automated analysis)
â””â”€ Recommendation: Batch similar tasks to reduce confirmation overhead

Task Completion Efficiency:
â”œâ”€ Avg subtasks per iteration: 3
â”œâ”€ Completion rate: 100% (0 failures)
â”œâ”€ Rework rate: 0% (no tasks returned from Review)
â”œâ”€ Quality score: 95% (discoveries per task)
```

### Quality Metrics

```text
DUPLICATE PREVENTION EFFECTIVENESS

Iteration 1:
â”œâ”€ Discovered improvements: 4
â”œâ”€ Checked for duplicates: 4
â”œâ”€ Duplicates found: 0
â””â”€ New tasks added: 4

Iteration 2:
â”œâ”€ Discovered improvements: 3
â”œâ”€ Checked for duplicates: 3
â”œâ”€ Duplicates found: 1 ("Add rate limiting" already exists)
â”‚  â””â”€ Action: Skipped
â””â”€ New tasks added: 2

Duplicate Prevention Summary:
â”œâ”€ Total improvements discovered: 7
â”œâ”€ Duplicates prevented: 1
â”œâ”€ Prevention rate: 14%
â”œâ”€ Effectiveness: âœ… WORKING
â””â”€ Note: Prevents task backlog pollution

Error Handling:
â”œâ”€ Tasks attempted: 6
â”œâ”€ Tasks succeeded: 6
â”œâ”€ Tasks failed: 0
â”œâ”€ Failure rate: 0%
â”œâ”€ All failures handled by Review
â””â”€ Recommendation: Continue current approach
```

### User Interaction

```text
CONFIRMATION PATTERNS

Decision Points per Iteration:
â”œâ”€ Smart Plan:
â”‚  â””â”€ "Ready to execute?" [1 decision point]
â”œâ”€ Smart Execute:
â”‚  â””â”€ "âœ… Task done?" [N decision points = N subtasks]
â””â”€ Smart Review:
   â””â”€ "Continue loop?" [1 decision point]

Iteration 1 Confirmations:
â”œâ”€ Plan phase: 1 (Ready? YES)
â”œâ”€ Execute phase: 4 (3 tasks + overall ready)
â”œâ”€ Review phase: 2 (Add tasks? YES â†’ Continue loop? YES)
â””â”€ Total: 7 confirmations in 4:30 (1 per 38 seconds)

User Response Time:
â”œâ”€ Quick decisions (< 5s): 5 confirmations (71%)
â”œâ”€ Medium decisions (5-15s): 2 confirmations (29%)
â”œâ”€ Slow decisions (> 15s): 0 confirmations (0%)
â””â”€ Avg response time: 4.2 seconds
```

---

## Live Dashboard View (Terminal Output)

When running the tight loop, your dashboard might look like:

```text
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                   TIGHT LOOP WORKFLOW DASHBOARD
                    Session ID: 2024-12-15-14:32
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š REAL-TIME STATUS
â”‚
â”œâ”€ Iteration: 1 of [TBD]
â”œâ”€ Current Phase: EXECUTE (Step 2/5)
â”œâ”€ Time Elapsed: 01:32
â”œâ”€ Last Update: 14:33:45
â”‚
â”œâ”€ Active Task: "Implement OAuth endpoints"
â”œâ”€ Status: IN PROGRESS
â”œâ”€ Completion: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80%
â””â”€ Awaiting: User confirmation: "âœ… OAuth setup done? [YES/NO]"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“ˆ AGGREGATE METRICS
â”‚
â”œâ”€ Phase Summary:
â”‚  â”œâ”€ Plan (Iteration 1):    COMPLETE (30s)
â”‚  â”œâ”€ Execute (Iteration 1): IN PROGRESS (21s of 45s est)
â”‚  â””â”€ Review (Iteration 1):  QUEUED
â”‚
â”œâ”€ Work Summary:
â”‚  â”œâ”€ Subtasks Created: 3
â”‚  â”œâ”€ Subtasks Completed: 2 of 3
â”‚  â”œâ”€ Subtasks Failed: 0
â”‚  â””â”€ Progress: 66%
â”‚
â””â”€ Discoveries:
   â”œâ”€ Issues Found: 2
   â”œâ”€ Duplicates Prevented: 0
   â””â”€ New Tasks Queued: [TBD]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¯ CURRENT DECISION POINT

Question: âœ… Subtask "Implement OAuth endpoints" complete?

Options:
  [Y] Mark complete, move to next subtask
  [N] Task failed, return to analysis
  [H] Help/details about this task

Status: AWAITING USER RESPONSE (max 5 minutes idle timeout)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“‹ RECENT ACTIVITY LOG

14:32:00 â”‚ Session Started
14:32:05 â”‚ âœ“ Loaded workflow context
14:32:08 â”‚ âœ“ Found task: "Implement Authentication System" (Priority: HIGH)
14:32:10 â”‚ Smart Plan: Analyzing vagueness...
14:32:18 â”‚ Smart Plan: Vagueness score = 0.6 (moderate)
14:32:22 â”‚ Smart Plan: Ask user clarifications [INTERACTION]
14:32:34 â”‚ âœ“ User answered: Scope includes OAuth + JWT
14:32:45 â”‚ Smart Plan: Creating subtasks...
14:32:52 â”‚ âœ“ Created 3 subtasks
14:33:02 â”‚ âœ“ User confirmed: Ready to execute? [YES]
14:33:05 â”‚ Handoff: Smart Plan â†’ Smart Execute
14:33:07 â”‚ Smart Execute: Loading context...
14:33:12 â”‚ Smart Execute: Executing Task 1 (OAuth endpoints)...
14:33:27 â”‚ âœ“ Task 1 complete
14:33:30 â”‚ âœ“ User confirmed: Mark complete? [YES]
14:33:33 â”‚ Smart Execute: Executing Task 2 (JWT management)...
14:33:42 â”‚ [WAITING FOR USER CONFIRMATION]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

```text
â±ï¸ ESTIMATED REMAINING TIME
â”œâ”€ Current task: ~8 seconds
â”œâ”€ Remaining execute subtasks: ~25 seconds
â”œâ”€ Review phase: ~45 seconds
â”œâ”€ Loop decision: ~5 seconds
â””â”€ Total remaining: ~1:23 minutes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Data Collection for Analysis

### What Gets Tracked

```text
Per Iteration:
â”œâ”€ Task ID planned
â”œâ”€ Subtask count
â”œâ”€ Subtask completion status (each)
â”œâ”€ Execution time per subtask
â”œâ”€ User confirmations (responses + timing)
â”œâ”€ Issues discovered
â”œâ”€ Duplicates found and skipped
â”œâ”€ Root causes identified
â”œâ”€ New tasks created
â””â”€ Loop decision (continue/break)

Per Session:
â”œâ”€ Session ID (timestamp)
â”œâ”€ Start/end time
â”œâ”€ Total iterations
â”œâ”€ Total tasks processed
â”œâ”€ Total improvements discovered
â”œâ”€ Duplicate prevention effectiveness
â”œâ”€ Success rate (% completed vs failed)
â”œâ”€ Average phase timing
â”œâ”€ User interaction patterns
â”œâ”€ Bottlenecks identified
â””â”€ Quality metrics
```

### Storage

Dashboard data gets logged to:

- **Memory:** `/memories/dev/smart-execute/` (per-iteration tracking)
- **Zen Tasks:** Observations field (task-level details)
- **Session Summary:** Full results when loop ends

---

## Agent Loop Dashboard Integration

### Smart Execute Phase - Updates

Agents should **READ** the current dashboard state, then **APPEND** updates:

```markdown
[Execution Update - HH:MM:SS]
**Current Task:** [Title] (Status: in-progress)
**Completed:** [Task A], [Task B], [Task C]
**Failed:** [Task X] (Error: brief reason)
**Metrics:** [X tasks/min], [timing]
**Notes:** [Short observation from this task]
```

### Smart Review Phase - Updates

Agents should **READ** execution updates, then **APPEND** review results:

```markdown
[Review Update - HH:MM:SS]
**Completed Count:** [N] tasks verified
**Failed Count:** [M] tasks analyzed
**Discovered Tasks:** [K] new tasks created
**Key Findings:** [Brief pattern summary]
**Recommendation:** [Replan | Continue | Done]
**Next Step:** [What happens next]
```

### Dashboard Format Rules

- Each phase appends its section (don't overwrite)
- Keep recent task list (last 5-10 items visible)
- Observations should be short (1-2 sentences max)
- Timestamp every update (HH:MM:SS format)
- Current task always at top of Execute section
- READ current state before updating (understand context)

---

## Monitoring Checklist

During testing, watch for these indicators:

### âœ… Healthy Indicators

- [ ] Each phase starts with fresh context load
- [ ] Smart Plan finds different task each iteration (via getNextTask)
- [ ] Smart Execute asks per-task confirmation
- [ ] Smart Review shows duplicate count (even if 0)
- [ ] Handoffs happen automatically (no manual switching)
- [ ] User responses are captured correctly
- [ ] Loop continues/breaks as expected
- [ ] Session summary shows all metrics

### ğŸš¨ Warning Indicators

- [ ] Same task planned twice (getNextTask not working)
- [ ] No per-task confirmations in Execute
- [ ] No duplicate count shown in Review
- [ ] Manual agent switching required
- [ ] User responses not captured
- [ ] Loop doesn't continue properly
- [ ] Session hangs during phase transition
- [ ] Missing data in final summary

### ğŸ”´ Critical Issues

- [ ] askuser cycles overlap between agents
- [ ] Agent state inherited from previous phase
- [ ] Duplicate tasks created (prevention failed)
- [ ] Tasks marked complete without user confirmation
- [ ] Loop infinite or terminates unexpectedly
- [ ] Session crashes during iteration

---

## Analysis Template (Post-Session)

After testing, fill out this analysis:

```markdown
# Test Session Analysis

**Session ID:** [TIMESTAMP]
**Tester:** [NAME]
**Date:** [DATE]
**Duration:** [MM:SS]

## Observations

**âœ… What Worked Well:**
- [Observation 1]
- [Observation 2]
- [Observation 3]

**âš ï¸ What Needs Improvement:**
- [Issue 1]: [Description] â†’ [Recommendation]
- [Issue 2]: [Description] â†’ [Recommendation]

**ğŸ”´ Critical Issues:**
- [Critical 1]: [Description] â†’ [Urgency: HIGH]
- [Critical 2]: [Description] â†’ [Urgency: HIGH]

## Metrics

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Iterations Completed | 2 | 2+ | âœ… PASS |
| Tasks Per Iteration | 3 | 2-5 | âœ… PASS |
| Completion Rate | 100% | 95%+ | âœ… PASS |
| Duplicate Prevention | 1/7 | 10%+ | âœ… PASS |
| Avg Phase Time | 40s | < 60s | âœ… PASS |

## Recommendations

1. [Priority 1] [Action]
2. [Priority 2] [Action]
3. [Priority 3] [Action]

## Next Steps

- [ ] Action 1
- [ ] Action 2
- [ ] Action 3
```

---

## Using This Dashboard

### During Testing

1. Keep this file open while running the tight loop
2. Manually update metrics as phases complete
3. Note any deviations from expected behavior
4. Record user interaction patterns

### After Testing

1. Fill out Analysis Template above
2. Identify patterns and bottlenecks
3. Create GitHub issues for improvements
4. Update documentation based on learnings

### For Multiple Sessions

1. Create a new dashboard per session
2. Compare metrics across sessions
3. Identify consistency and regressions
4. Track improvement over time

---

## Quick Reference

**Dashboard Components:**

- Session metadata (ID, time, iteration count)
- Current phase status (step, time spent, awaiting)
- Aggregated metrics (tasks, time, quality)
- User interaction log (decisions, timing)
- Activity timeline (what happened, when)
- Estimated remaining time
- Live warnings for issues

**Key Metrics to Track:**

- Phase timing (are they consistent?)
- Completion rates (are tasks getting done?)
- Duplicate prevention (is it working?)
- User interaction (are they confirming quickly?)
- Loop behavior (does it continue/break correctly?)
- Error handling (are failures caught?)
- Overall quality (are improvements being discovered?)

**Success Criteria:**

- All phases complete without manual switching
- Duplicate prevention shows positive counts (issues prevented)
- User confirmations happen naturally
- Loop continues as expected
- Session summary is complete and accurate

---

## Observations

### Agent Role Focus (CRITICAL)

Each agent focuses ONLY on its assigned jobâ€”no cross-role responsibilities:

- **Smart Plan:** Creates subtasks in Zen Tasks (plans, doesn't execute or review)
- **Smart Execute:** Executes tasks handed to it (reads task names/docs, applies properly, doesn't mark done)
- **Smart Review:** Reviews execution results, creates discovered tasks, **marks completed tasks as done** (doesn't execute)

### Task Completion Flow (CRITICAL)

Tasks must NEVER be marked complete until Smart Review reviews and confirms:

1. Smart Execute runs tasks but does NOT mark them complete
2. Smart Review analyzes results, confirms work quality, then marks tasks as done
3. This ensures accountability and prevents premature completion claims

### Use Ask User for (Developer Interaction)

- **Critical non-trivial decisions** - Architecture choices, scope changes, requirement conflicts
- **Program verification** - "Has the program been started?" / "Is the GUI accessible?"
- **Automated verification not possible** - Manual testing requirements, non-deterministic features
- **GUI/UX feedback** - "Review this page element by element, button by buttonâ€”what's missing?"
- **Post-execution validation** - "Does this functionality work as expected through the UI?"
- **Clarifications when** - Info not in plan/tasks/project documentation

**Example Ask User Scenarios:**

- Execute phase: "Program startedâ€”can you verify the login flow through the GUI?"
- Review phase: "Page-by-page review needed: [URL or feature]. Anything missing or need changes?"
- Plan phase: "Requirement ambiguousâ€”what does 'responsive design' mean in your context?"

### Proceed WITHOUT extra prompts when

- Executing clearly scoped subtasks already approved in plan
- Continuing deterministic actions within an in-progress phase
- Loading context, fetching tasks, running analysis
- Updating loop status dashboard or logging observations
- Performing automated duplicate prevention, validation, pattern analysis
- Creating discovered tasks from review observations (automatic, no prompting)


### Dashboard updates (execution + review)

- Smart Execute updates live loop dashboard metrics while executing (status, counts, confirmations)
- Smart Review updates dashboard with discoveries, failures, duplicate-prevention counts, and completion summaries
- Keep dashboard in sync each phase; no waiting for hub handoff

**note:** Goal = **Chaos coding with minimum developer interference, maximum autopilot**. Ask User only for: non-trivial decisions, GUI verification, feedback that requires human judgment. Everything else runs automatically.

- **Monitor for:**
  - Repeated tasks being planned (getNextTask issue or cycle detection failure)
  - Smart Execute marking tasks complete (should NOT do thisâ€”only Smart Review)
  - No duplicate prevention in Review phase (discovered tasks creating duplicates)
  - Manual agent switching (should route via Full Auto buttons)
  - Missing or delayed user responses to critical asks (clarifications, GUI verification)
  - Loop not continuing or breaking as expected
  - Session hanging during phase transitions
  - Incomplete data in final summary
  - Each agent has full access to previous agent's chat history for context
